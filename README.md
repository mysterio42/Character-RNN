
```shell
python run.py
```
```shell
  --lr LR        Model learning rate default: 0.1
  --load LOAD    True: Load trained model False: Train model default: True
```

### Recurrent (ReLU Activation, One Layer, 200 epochs)
```text
Epoch: 10/200 Loss:2.5239672660827637
Epoch: 20/200 Loss:2.173161029815674
Epoch: 30/200 Loss:1.794061541557312
Epoch: 40/200 Loss:1.2868115901947021
Epoch: 50/200 Loss:0.7460516691207886
Epoch: 60/200 Loss:0.29406028985977173
Epoch: 70/200 Loss:0.1075957641005516
Epoch: 80/200 Loss:0.061154939234256744
Epoch: 90/200 Loss:0.047639183700084686
Epoch: 100/200 Loss:0.042270224541425705
Epoch: 110/200 Loss:0.03957294672727585
Epoch: 120/200 Loss:0.038035064935684204
Epoch: 130/200 Loss:0.03706770017743111
Epoch: 140/200 Loss:0.036404822021722794
Epoch: 150/200 Loss:0.03591925650835037
Epoch: 160/200 Loss:0.03554418310523033
Epoch: 170/200 Loss:0.035245660692453384
Epoch: 180/200 Loss:0.03500327467918396
Epoch: 190/200 Loss:0.03480077162384987
Epoch: 200/200 Loss:0.03462965041399002
```

